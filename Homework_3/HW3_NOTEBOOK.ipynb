{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.2\n",
      "  Downloading nltk-3.2.tar.gz (1.2MB)\n",
      "\u001b[K    100% |################################| 1.2MB 363kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk\n",
      "  Stored in directory: /root/.cache/pip/wheels/43/dd/39/f0155c88c9188dda43977fbb04c8c0cf9e61ca82dd10135b52\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.2\n",
      "\u001b[33mYou are using pip version 7.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade nltk==3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data.csv\n",
    "который получился в результате предобработки данных, может быть скачан отсюда: https://yadi.sk/d/zPFaJh7m33HsMQ\n",
    "просто я работала в everware, который в результате по каким-то только ему известным причинам не смог запушить все обратно в репозиторий, поэтому изначальные данные и data.csv я в репозиторий загрузить не смогла. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.22822210732570425\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500          0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape, df.is_blocked.mean())\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.22822210732570425\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print(\"Blocked ratio\",df.is_blocked.mean())\n",
    "print(\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какие есть категории товаров и сколько товаров представлено в каждой из них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Недвижимость           287778\n",
       "Транспорт              185114\n",
       "Услуги                 173025\n",
       "Личные вещи            167788\n",
       "Бытовая электроника    101131\n",
       "Хобби и отдых           92142\n",
       "Работа                  77777\n",
       "Для дома и дачи         61298\n",
       "Животные                32376\n",
       "Для бизнеса             26520\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждой категории посмотрим, сколько в ней blocked и not blocked товаров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бытовая электроника\n",
      "0    93621\n",
      "1     7510\n",
      "Name: is_blocked, dtype: int64\n",
      "\n",
      "Для бизнеса\n",
      "0    13509\n",
      "1    13011\n",
      "Name: is_blocked, dtype: int64\n",
      "\n",
      "Для дома и дачи\n",
      "0    54399\n",
      "1     6899\n",
      "Name: is_blocked, dtype: int64\n",
      "\n",
      "Животные\n",
      "0    29027\n",
      "1     3349\n",
      "Name: is_blocked, dtype: int64\n",
      "\n",
      "Личные вещи\n",
      "0    144348\n",
      "1     23440\n",
      "Name: is_blocked, dtype: int64\n",
      "\n",
      "Недвижимость\n",
      "0    286351\n",
      "1      1427\n",
      "Name: is_blocked, dtype: int64\n",
      "\n",
      "Работа\n",
      "0    57934\n",
      "1    19843\n",
      "Name: is_blocked, dtype: int64\n",
      "\n",
      "Транспорт\n",
      "0    167078\n",
      "1     18036\n",
      "Name: is_blocked, dtype: int64\n",
      "\n",
      "Услуги\n",
      "1    124402\n",
      "0     48623\n",
      "Name: is_blocked, dtype: int64\n",
      "\n",
      "Хобби и отдых\n",
      "1    57079\n",
      "0    35063\n",
      "Name: is_blocked, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category in np.unique(df['category']):\n",
    "    print(category)\n",
    "    print(df[df.category == category].is_blocked.value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте просто сделаем так, чтобы в каждой категории было поровну is_blocked и не is_blocked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.5\n",
      "Count: 354402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tatiana/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py:1997: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"DataFrame index.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "indices_to_delete = []\n",
    "for category in np.unique(df['category']):\n",
    "    value_counts = df[df.category == category].is_blocked.value_counts()\n",
    "    num_to_delete = abs(value_counts[0] - value_counts[1])\n",
    "    what_to_delete = 0 if value_counts[0] - value_counts[1] > 0 else 1\n",
    "        \n",
    "    indices_to_delete += list(np.random.permutation(df[df.category == category]\n",
    "                                                    [df.is_blocked == what_to_delete].index)[:num_to_delete])\n",
    "        \n",
    "df.drop(df.index[indices_to_delete], inplace=True)\n",
    "        \n",
    "\n",
    "print(\"Blocked ratio:\",df.is_blocked.mean())\n",
    "print(\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In case your RAM-o-meter is in the red\n",
    "if very_low_RAM:\n",
    "    data = data[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10000700</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Часы и украшения</td>\n",
       "      <td>Бусы ассорти (натуральные камни)</td>\n",
       "      <td>h-52 см</td>\n",
       "      <td>{\"Вид товара\":\"Ювелирные изделия\"}</td>\n",
       "      <td>700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10001842</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Запчасти и аксессуары</td>\n",
       "      <td>Датчик массового расхода воздуха (дмрв) Volvo S40</td>\n",
       "      <td>Продам оригинальный ДМРВ Volvo, стоял на модел...</td>\n",
       "      <td>{\"Вид товара\":\"Запчасти\", \"Тип товара\":\"Для ав...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10002207</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Контролные, рефераты, дипломы, курсовые и т. д</td>\n",
       "      <td>Контрольные , рефераты , курсовые дипломы!!!! ...</td>\n",
       "      <td>{\"Вид услуги\":\"Образование, курсы\"}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10002609</td>\n",
       "      <td>Хобби и отдых</td>\n",
       "      <td>Билеты и путешествия</td>\n",
       "      <td>Прага-Мюнхен-Замки Баварии-Дрезден из Краснодара</td>\n",
       "      <td>Предложение действительно с 10 ноября по 23 фе...</td>\n",
       "      <td>{\"Вид товара\":\"Путешествия\"}</td>\n",
       "      <td>22400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10002838</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Зимний конверт на выписку</td>\n",
       "      <td>Теплый конверт на выписку для девочки нежно ро...</td>\n",
       "      <td>{\"Вид одежды\":\"Для девочек\", \"Предмет одежды\":...</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      itemid       category             subcategory  \\\n",
       "10  10000700    Личные вещи        Часы и украшения   \n",
       "17  10001842      Транспорт   Запчасти и аксессуары   \n",
       "21  10002207         Услуги       Предложения услуг   \n",
       "24  10002609  Хобби и отдых    Билеты и путешествия   \n",
       "26  10002838    Личные вещи  Детская одежда и обувь   \n",
       "\n",
       "                                                title  \\\n",
       "10                   Бусы ассорти (натуральные камни)   \n",
       "17  Датчик массового расхода воздуха (дмрв) Volvo S40   \n",
       "21     Контролные, рефераты, дипломы, курсовые и т. д   \n",
       "24   Прага-Мюнхен-Замки Баварии-Дрезден из Краснодара   \n",
       "26                          Зимний конверт на выписку   \n",
       "\n",
       "                                          description  \\\n",
       "10                                            h-52 см   \n",
       "17  Продам оригинальный ДМРВ Volvo, стоял на модел...   \n",
       "21  Контрольные , рефераты , курсовые дипломы!!!! ...   \n",
       "24  Предложение действительно с 10 ноября по 23 фе...   \n",
       "26  Теплый конверт на выписку для девочки нежно ро...   \n",
       "\n",
       "                                                attrs  price  is_proved  \\\n",
       "10                 {\"Вид товара\":\"Ювелирные изделия\"}    700        NaN   \n",
       "17  {\"Вид товара\":\"Запчасти\", \"Тип товара\":\"Для ав...   3000        NaN   \n",
       "21                {\"Вид услуги\":\"Образование, курсы\"}      0          0   \n",
       "24                       {\"Вид товара\":\"Путешествия\"}  22400        NaN   \n",
       "26  {\"Вид одежды\":\"Для девочек\", \"Предмет одежды\":...    250        NaN   \n",
       "\n",
       "    is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "10           0           0           0         0         2.28  \n",
       "17           0           0           0         0        18.78  \n",
       "21           1           1           1         0         0.16  \n",
       "24           0           0           0         0         0.03  \n",
       "26           0           0           0         0         1.65  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10000700</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Часы и украшения</td>\n",
       "      <td>Бусы ассорти (натуральные камни)</td>\n",
       "      <td>h-52 см</td>\n",
       "      <td>{\"Вид товара\":\"Ювелирные изделия\"}</td>\n",
       "      <td>700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>10001842</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Запчасти и аксессуары</td>\n",
       "      <td>Датчик массового расхода воздуха (дмрв) Volvo S40</td>\n",
       "      <td>Продам оригинальный ДМРВ Volvo, стоял на модел...</td>\n",
       "      <td>{\"Вид товара\":\"Запчасти\", \"Тип товара\":\"Для ав...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>10002207</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Контролные, рефераты, дипломы, курсовые и т. д</td>\n",
       "      <td>Контрольные , рефераты , курсовые дипломы!!!! ...</td>\n",
       "      <td>{\"Вид услуги\":\"Образование, курсы\"}</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>10002609</td>\n",
       "      <td>Хобби и отдых</td>\n",
       "      <td>Билеты и путешествия</td>\n",
       "      <td>Прага-Мюнхен-Замки Баварии-Дрезден из Краснодара</td>\n",
       "      <td>Предложение действительно с 10 ноября по 23 фе...</td>\n",
       "      <td>{\"Вид товара\":\"Путешествия\"}</td>\n",
       "      <td>22400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>10002838</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Зимний конверт на выписку</td>\n",
       "      <td>Теплый конверт на выписку для девочки нежно ро...</td>\n",
       "      <td>{\"Вид одежды\":\"Для девочек\", \"Предмет одежды\":...</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    itemid       category             subcategory  \\\n",
       "0          10  10000700    Личные вещи        Часы и украшения   \n",
       "1          17  10001842      Транспорт   Запчасти и аксессуары   \n",
       "2          21  10002207         Услуги       Предложения услуг   \n",
       "3          24  10002609  Хобби и отдых    Билеты и путешествия   \n",
       "4          26  10002838    Личные вещи  Детская одежда и обувь   \n",
       "\n",
       "                                               title  \\\n",
       "0                   Бусы ассорти (натуральные камни)   \n",
       "1  Датчик массового расхода воздуха (дмрв) Volvo S40   \n",
       "2     Контролные, рефераты, дипломы, курсовые и т. д   \n",
       "3   Прага-Мюнхен-Замки Баварии-Дрезден из Краснодара   \n",
       "4                          Зимний конверт на выписку   \n",
       "\n",
       "                                         description  \\\n",
       "0                                            h-52 см   \n",
       "1  Продам оригинальный ДМРВ Volvo, стоял на модел...   \n",
       "2  Контрольные , рефераты , курсовые дипломы!!!! ...   \n",
       "3  Предложение действительно с 10 ноября по 23 фе...   \n",
       "4  Теплый конверт на выписку для девочки нежно ро...   \n",
       "\n",
       "                                               attrs  price  is_proved  \\\n",
       "0                 {\"Вид товара\":\"Ювелирные изделия\"}    700        NaN   \n",
       "1  {\"Вид товара\":\"Запчасти\", \"Тип товара\":\"Для ав...   3000        NaN   \n",
       "2                {\"Вид услуги\":\"Образование, курсы\"}      0          0   \n",
       "3                       {\"Вид товара\":\"Путешествия\"}  22400        NaN   \n",
       "4  {\"Вид одежды\":\"Для девочек\", \"Предмет одежды\":...    250        NaN   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         2.28  \n",
       "1           0           0           0         0        18.78  \n",
       "2           1           1           1         0         0.16  \n",
       "3           0           0           0         0         0.03  \n",
       "4           0           0           0         0         1.65  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.decode('utf8').lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEACAYAAABCl1qQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSeoyLCRJXYaFJKnLsJAkdRkWkqSu/w9O5ABSZYPh7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb6366206d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_=plt.hist(token_counts.values(),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "min_count = 10\n",
    "tokens = [token for token in token_counts.keys() if token_counts[token] >= 10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 72408\n"
     ]
    }
   ],
   "source": [
    "print \"# Tokens:\",len(token_to_id)\n",
    "if len(token_to_id) < 30000:\n",
    "    print \"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\"\n",
    "if len(token_to_id) > 1000000:\n",
    "    print \"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.decode('utf8').lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = map(lambda token: token_to_id.get(token,0), tokens)[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (354402, 15)\n",
      "Бусы ассорти (натуральные камни) -> [63828 39208 20984 61748     0     0     0     0     0     0] ...\n",
      "Датчик массового расхода воздуха (дмрв) Volvo S40 -> [64620 50225 15166 37454 20023 29845 50650     0     0     0] ...\n",
      "Контролные, рефераты, дипломы, курсовые и т. д -> [    0 29753  8654 46571 71927  3052  3702     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Размер матрицы:\",title_tokens.shape\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print title,'->', tokens[:10],'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "\n",
    "categories = [{\"category\":category_name, \"subcategory\":subcategory_name} \n",
    "              for category_name, subcategory_name in data_cat_subcat]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "\n",
    "\n",
    "#Difficulty selector:\n",
    "#Easy: split randomly\n",
    "#Medium: select test set items that have item_ids strictly above that of training set\n",
    "#Hard: do whatever you want, but score yourself using kaggle private leaderboard\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "data_tuple = train_test_split(title_tokens,desc_tokens,df_non_text.values,target)\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading saved data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "save_prepared_data = False #save\n",
    "read_prepared_data = True #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print \"Saving preprocessed data (may take up to 3 minutes)\"\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print \"готово\"\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print \"Reading saved data...\"\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "   \n",
    "    print \"done\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/Theano/Theano/archive/master.zip\n",
      "  Downloading https://github.com/Theano/Theano/archive/master.zip\n",
      "\u001b[K     - 13.6MB 42.5MB/s\n",
      "Requirement already up-to-date: numpy>=1.9.1 in /root/miniconda/envs/rep_py2/lib/python2.7/site-packages (from Theano==0.9.0.dev4)\n",
      "Collecting scipy>=0.14 (from Theano==0.9.0.dev4)\n",
      "  Downloading scipy-0.18.1.tar.gz (13.1MB)\n",
      "\u001b[K    100% |################################| 13.1MB 41kB/s \n",
      "\u001b[?25hRequirement already up-to-date: six>=1.9.0 in /root/miniconda/envs/rep_py2/lib/python2.7/site-packages (from Theano==0.9.0.dev4)\n",
      "Building wheels for collected packages: scipy\n",
      "  Running setup.py bdist_wheel for scipy\n",
      "  Stored in directory: /root/.cache/pip/wheels/33/c4/f5/e00fe242696eba9e5f63cd0f30eaf5780b8c98067eb164707c\n",
      "Successfully built scipy\n",
      "Installing collected packages: scipy, Theano\n",
      "  Found existing installation: scipy 0.16.0\n",
      "\u001b[33m    DEPRECATION: Uninstalling a distutils installed project (scipy) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\u001b[0m\n",
      "    Uninstalling scipy-0.16.0:\n",
      "      Successfully uninstalled scipy-0.16.0\n",
      "  Found existing installation: Theano 0.7.0\n",
      "    Uninstalling Theano-0.7.0:\n",
      "      Successfully uninstalled Theano-0.7.0\n",
      "  Running setup.py install for Theano\n",
      "Successfully installed Theano-0.9.0.dev4 scipy-0.18.1\n",
      "\u001b[33mYou are using pip version 7.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting https://github.com/Lasagne/Lasagne/archive/master.zip\n",
      "  Downloading https://github.com/Lasagne/Lasagne/archive/master.zip (218kB)\n",
      "\u001b[K    100% |################################| 221kB 274kB/s \n",
      "\u001b[?25hRequirement already up-to-date: numpy in /root/miniconda/envs/rep_py2/lib/python2.7/site-packages (from Lasagne==0.2.dev1)\n",
      "Installing collected packages: Lasagne\n",
      "  Found existing installation: Lasagne 0.2.dev1\n",
      "    Uninstalling Lasagne-0.2.dev1:\n",
      "      Successfully uninstalled Lasagne-0.2.dev1\n",
      "  Running setup.py install for Lasagne\n",
      "Successfully installed Lasagne-0.2.dev1\n",
      "\u001b[33mYou are using pip version 7.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade https://github.com/Theano/Theano/archive/master.zip\n",
    "! pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=128)\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn, 30, only_return_final=True)\n",
    "\n",
    "# Titles\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp,input_size=len(token_to_id)+1,output_size=128)\n",
    "title_nn = lasagne.layers.Conv1DLayer(title_nn, num_filters=50, filter_size=5)\n",
    "title_nn = lasagne.layers.MaxPool1DLayer(title_nn, pool_size=2)\n",
    "title_nn = lasagne.layers.DenseLayer(title_nn, num_units=50)\n",
    "\n",
    "# Non-sequences\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, num_units=100)\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_nn, num_units=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.concat([descr_nn, title_nn, cat_nn])\n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn, 1024)\n",
    "nn = lasagne.layers.DropoutLayer(nn, p=0.15)\n",
    "nn = lasagne.layers.DenseLayer(nn, 1, nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction,target_y,delta = 1.).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.adadelta(loss, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction,target_y,delta = 1.0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренировала по 20 эпох, потом смотрела на скор. Остановлась, когда получился норм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "    loss: 0.1039793197501\n",
      "    acc: 0.962548573367\n",
      "    auc: 0.984580604749\n",
      "    ap@k: 0.990745895759\n",
      "Val:\n",
      "    loss: 0.120036583995\n",
      "    acc: 0.946395384890\n",
      "    auc: 0.976906568009\n",
      "    ap@k: 0.989490336234\n",
      "Train:\n",
      "    loss: 0.0099989199353\n",
      "    acc: 0.967593659608\n",
      "    auc: 0.988407437990\n",
      "    ap@k: 0.991037506063\n",
      "Val:\n",
      "    loss: 0.104037596794\n",
      "    acc: 0.950100468593\n",
      "    auc: 0.979938694875\n",
      "    ap@k: 0.990873675995\n",
      "Train:\n",
      "    loss: 0.100007692004\n",
      "    acc: 0.959036859690\n",
      "    auc: 0.983895697699\n",
      "    ap@k: 0.989671959769\n",
      "Val:\n",
      "    loss: 0.109957258786\n",
      "    acc: 0.949037876986\n",
      "    auc: 0.975973687899\n",
      "    ap@k: 0.989962858060\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Train:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Val:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "        loss: 0.119983658028\n",
      "        acc: 0.932805683958\n",
      "        auc: 0.969663906794\n",
      "        ap@k: 0.982975686393\n",
      "\n",
      "AUC:\n",
      "        Отличное решение! (good)\n",
      "\n",
      "Accuracy:\n",
      "        Всё ок (ok)\n",
      "\n",
      "Average precision at K:\n",
      "        Отличный результат (good)\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
