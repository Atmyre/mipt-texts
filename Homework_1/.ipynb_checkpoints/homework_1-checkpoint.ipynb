{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #1, 2 Считывание данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачав данные и посмотрев на них, понятно, что delimiter='\\t':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./SMSSpamCollection.txt', names=['label', 'content'], delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #3 Подготовка двух списков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "       'Ok lar... Joking wif u oni...',\n",
       "       \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       "       ..., 'Pity, * was in mood for that. So...any other suggestions?',\n",
       "       \"The guy did some bitching but I acted like i'd be interested in buying something else next week and he gave it to us for free\",\n",
       "       'Rofl. Its true to its name'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = data.content.values\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = data.label.values\n",
    "labels = np.array([1 if labels[i] == 'spam' else 0 for i in range(len(labels))])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13406317300789664"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(labels) / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не спама, как видно, очень мало. Наверняка алгоритмы переобучатся на спам. Посмотрим, какой меньше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #4 Получение матрицы признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x8713 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 74169 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(texts)\n",
    "texts_matrix = cv.transform(texts)\n",
    "\n",
    "texts_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #5 LogisticRegression() и sklearn.cross_validation.cross_val_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9326402983610631"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "score = np.mean(cross_val_score(lr, texts_matrix, labels, cv=10, scoring='f1'))\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #6 Обучение на всем и ответ для сообщений из задания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_texts = [\"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GB\", \n",
    "             \"FreeMsg: Txt: claim your reward of 3 hours talk time\", \n",
    "             \"Have you visited the last lecture on physics?\", \n",
    "             \"Have you visited the last lecture on physics? Just buy this book and you will have all materials! Only 99$\", \n",
    "             \"Only 99$\"]\n",
    "test_texts_matrix = cv.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(texts_matrix, labels)\n",
    "lr.predict(test_texts_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мысли по этому поводу в пункте 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #7 LogisticRegression с *граммами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for ngram_range in [(2, 2), (3, 3), (1, 3)]: \n",
    "    cv = CountVectorizer(ngram_range=ngram_range)\n",
    "    cv.fit(texts)\n",
    "    texts_matrix = cv.transform(texts)\n",
    "    lr = LogisticRegression()\n",
    "    scores.append(np.mean(cross_val_score(lr, texts_matrix, labels, cv=10, scoring='f1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82 0.73 0.93\n"
     ]
    }
   ],
   "source": [
    "print(\"%.2f %.2f %.2f\" % (scores[0], scores[1], scores[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #8 MultinomialNB с *граммами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for ngram_range in [(2, 2), (3, 3), (1, 3)]: \n",
    "    cv = CountVectorizer(ngram_range=ngram_range)\n",
    "    cv.fit(texts)\n",
    "    texts_matrix = cv.transform(texts)\n",
    "    lr = MultinomialNB()\n",
    "    scores.append(np.mean(cross_val_score(lr, texts_matrix, labels, cv=10, scoring='f1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65 0.38 0.89\n"
     ]
    }
   ],
   "source": [
    "print(\"%.2f %.2f %.2f\" % (scores[0], scores[1], scores[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #9  TfidfVectorizer на униграммах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFiDF:  0.878510045534\n",
      "CV:  0.932640298361\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline    \n",
    "\n",
    "tfidf_model = make_pipeline (TfidfVectorizer(ngram_range=(1, 1)), LogisticRegression())\n",
    "print(\"TFiDF: \", np.mean(cross_val_score(tfidf_model, texts, labels, cv=10, scoring='f1')))\n",
    "\n",
    "cv_model = make_pipeline (CountVectorizer(ngram_range=(1, 1)), LogisticRegression())\n",
    "print(\"CV: \", np.mean(cross_val_score(cv_model, texts, labels, cv=10, scoring='f1')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv.fit(texts)\n",
    "texts_matrix = cv.transform(texts).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer + XGBClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6845           19.47m\n",
      "         2           0.6255           17.84m\n",
      "         3           0.5846           16.67m\n",
      "         4           0.5492           15.80m\n",
      "         5           0.5202           15.25m\n",
      "         6           0.4980           14.81m\n",
      "         7           0.4762           14.45m\n",
      "         8           0.4594           14.16m\n",
      "         9           0.4440           14.01m\n",
      "        10           0.4306           13.78m\n",
      "        20           0.3344           11.90m\n",
      "        30           0.2791           10.32m\n",
      "        40           0.2389            8.83m\n",
      "        50           0.2111            7.36m\n",
      "        60           0.1921            5.89m\n",
      "        70           0.1784            4.42m\n",
      "        80           0.1667            2.94m\n",
      "        90           0.1483            1.47m\n",
      "       100           0.1394            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6845           15.93m\n",
      "         2           0.6252           15.85m\n",
      "         3           0.5838           15.67m\n",
      "         4           0.5489           15.50m\n",
      "         5           0.5229           15.34m\n",
      "         6           0.5005           15.17m\n",
      "         7           0.4781           15.00m\n",
      "         8           0.4612           14.85m\n",
      "         9           0.4430           14.70m\n",
      "        10           0.4301           14.54m\n",
      "        20           0.3329           12.97m\n",
      "        30           0.2773           11.02m\n",
      "        40           0.2394            9.26m\n",
      "        50           0.2085            7.64m\n",
      "        60           0.1868            6.07m\n",
      "        70           0.1713            4.52m\n",
      "        80           0.1581            2.99m\n",
      "        90           0.1408            1.48m\n",
      "       100           0.1331            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6868           13.32m\n",
      "         2           0.6281           13.17m\n",
      "         3           0.5841           13.03m\n",
      "         4           0.5523           12.87m\n",
      "         5           0.5255           12.72m\n",
      "         6           0.5005           12.58m\n",
      "         7           0.4794           12.45m\n",
      "         8           0.4623           12.31m\n",
      "         9           0.4473           12.17m\n",
      "        10           0.4341           12.04m\n",
      "        20           0.3366           10.68m\n",
      "        30           0.2793            9.33m\n",
      "        40           0.2424            8.00m\n",
      "        50           0.2153            6.65m\n",
      "        60           0.1952            5.29m\n",
      "        70           0.1800            3.96m\n",
      "        80           0.1673            2.63m\n",
      "        90           0.1493            1.31m\n",
      "       100           0.1391            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6854           13.27m\n",
      "         2           0.6286           13.10m\n",
      "         3           0.5867           12.93m\n",
      "         4           0.5519           13.52m\n",
      "         5           0.5262           13.80m\n",
      "         6           0.5035           14.09m\n",
      "         7           0.4833           13.72m\n",
      "         8           0.4655           13.39m\n",
      "         9           0.4508           13.10m\n",
      "        10           0.4389           12.83m\n",
      "        20           0.3428           10.96m\n",
      "        30           0.2849            9.46m\n",
      "        40           0.2446            8.07m\n",
      "        50           0.2167            6.71m\n",
      "        60           0.1959            5.40m\n",
      "        70           0.1780            3.94m\n",
      "        80           0.1666            2.46m\n",
      "        90           0.1479            1.16m\n",
      "       100           0.1400            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6836            6.46m\n",
      "         2           0.6262            6.40m\n",
      "         3           0.5837            6.35m\n",
      "         4           0.5482            6.29m\n",
      "         5           0.5193            6.22m\n",
      "         6           0.4967            6.16m\n",
      "         7           0.4769            6.11m\n",
      "         8           0.4579            6.04m\n",
      "         9           0.4428            5.98m\n",
      "        10           0.4286            5.92m\n",
      "        20           0.3329            5.23m\n",
      "        30           0.2794            5.14m\n",
      "        40           0.2395            4.65m\n",
      "        50           0.2101            4.00m\n",
      "        60           0.1898            3.27m\n",
      "        70           0.1739            2.44m\n",
      "        80           0.1557            1.57m\n",
      "        90           0.1425           46.05s\n",
      "       100           0.1340            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6846            6.26m\n",
      "         2           0.6258            6.21m\n",
      "         3           0.5845            6.14m\n",
      "         4           0.5497            6.08m\n",
      "         5           0.5228            6.01m\n",
      "         6           0.4993            5.94m\n",
      "         7           0.4803            5.88m\n",
      "         8           0.4610            5.82m\n",
      "         9           0.4460            5.76m\n",
      "        10           0.4329            5.69m\n",
      "        20           0.3364            5.06m\n",
      "        30           0.2776            4.46m\n",
      "        40           0.2400            3.83m\n",
      "        50           0.2116            3.17m\n",
      "        60           0.1874            2.51m\n",
      "        70           0.1698            1.88m\n",
      "        80           0.1586            1.25m\n",
      "        90           0.1441           37.42s\n",
      "       100           0.1347            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6837            6.41m\n",
      "         2           0.6241            7.27m\n",
      "         3           0.5825            7.47m\n",
      "         4           0.5506            7.59m\n",
      "         5           0.5238            7.55m\n",
      "         6           0.4971            7.52m\n",
      "         7           0.4779            7.47m\n",
      "         8           0.4580            7.41m\n",
      "         9           0.4433            7.35m\n",
      "        10           0.4275            7.27m\n",
      "        20           0.3315            6.54m\n",
      "        30           0.2768            5.84m\n",
      "        40           0.2375            5.05m\n",
      "        50           0.2107            4.23m\n",
      "        60           0.1889            3.39m\n",
      "        70           0.1738            2.54m\n",
      "        80           0.1575            1.69m\n",
      "        90           0.1436           50.59s\n",
      "       100           0.1347            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6853            8.40m\n",
      "         2           0.6256            8.32m\n",
      "         3           0.5815            8.24m\n",
      "         4           0.5489            8.15m\n",
      "         5           0.5225            8.08m\n",
      "         6           0.4956            8.13m\n",
      "         7           0.4772            8.01m\n",
      "         8           0.4589            7.85m\n",
      "         9           0.4418            7.73m\n",
      "        10           0.4275            7.61m\n",
      "        20           0.3280            6.62m\n",
      "        30           0.2733            5.74m\n",
      "        40           0.2367            4.92m\n",
      "        50           0.2070            4.14m\n",
      "        60           0.1879            3.33m\n",
      "        70           0.1726            2.51m\n",
      "        80           0.1602            1.68m\n",
      "        90           0.1440           50.22s\n",
      "       100           0.1346            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6833            8.42m\n",
      "         2           0.6235            8.31m\n",
      "         3           0.5819            7.60m\n",
      "         4           0.5466            7.15m\n",
      "         5           0.5195            6.84m\n",
      "         6           0.4958            6.63m\n",
      "         7           0.4771            6.46m\n",
      "         8           0.4575            6.31m\n",
      "         9           0.4402            6.19m\n",
      "        10           0.4269            6.07m\n",
      "        20           0.3296            5.89m\n",
      "        30           0.2749            5.37m\n",
      "        40           0.2333            4.70m\n",
      "        50           0.2069            3.88m\n",
      "        60           0.1873            2.99m\n",
      "        70           0.1721            2.19m\n",
      "        80           0.1576            1.43m\n",
      "        90           0.1411           42.34s\n",
      "       100           0.1326            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6871            6.26m\n",
      "         2           0.6294            6.21m\n",
      "         3           0.5889            6.17m\n",
      "         4           0.5541            6.15m\n",
      "         5           0.5275            6.38m\n",
      "         6           0.5028            6.63m\n",
      "         7           0.4816            6.73m\n",
      "         8           0.4649            6.80m\n",
      "         9           0.4516            6.82m\n",
      "        10           0.4361            6.82m\n",
      "        20           0.3396            6.36m\n",
      "        30           0.2853            5.70m\n",
      "        40           0.2448            4.90m\n",
      "        50           0.2176            4.21m\n",
      "        60           0.1959            3.38m\n",
      "        70           0.1807            2.53m\n",
      "        80           0.1685            1.68m\n",
      "        90           0.1487           50.51s\n",
      "       100           0.1398            0.00s\n"
     ]
    }
   ],
   "source": [
    "score = np.mean(cross_val_score(GradientBoostingClassifier(n_estimators=100, verbose=True), \n",
    "                                texts_matrix, labels, cv=10, scoring='f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88518514175434837"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer + Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   15.3s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   29.6s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   13.7s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   28.6s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   14.3s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   29.8s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   15.9s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   30.5s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   14.2s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   29.6s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   13.6s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   29.3s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   14.3s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   29.2s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   13.6s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   28.2s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   14.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   27.6s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   13.6s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   27.5s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "score = np.mean(cross_val_score(RandomForestClassifier(n_estimators=100, verbose=True), \n",
    "                                texts_matrix, labels, cv=10, scoring='f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90988081359919448"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer + DecisionTreeClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = np.mean(cross_val_score(DecisionTreeClassifier(), \n",
    "                                texts_matrix, labels, cv=10, scoring='f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88728049422989963"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем те же самые классификаторы с Tfidf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(texts)\n",
    "texts_matrix_tfidf = tfidf.transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer + XGBClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6845           10.78m\n",
      "         2           0.6255            9.31m\n",
      "         3           0.5846            8.60m\n",
      "         4           0.5492            8.17m\n",
      "         5           0.5202            8.35m\n",
      "         6           0.4980            8.41m\n",
      "         7           0.4762            8.17m\n",
      "         8           0.4594            8.06m\n",
      "         9           0.4440            7.90m\n",
      "        10           0.4306            7.74m\n",
      "        20           0.3344            6.94m\n",
      "        30           0.2791            6.70m\n",
      "        40           0.2400            5.93m\n",
      "        50           0.2109            4.96m\n",
      "        60           0.1917            4.02m\n",
      "        70           0.1777            2.91m\n",
      "        80           0.1649            1.89m\n",
      "        90           0.1461           56.28s\n",
      "       100           0.1383            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6845            7.76m\n",
      "         2           0.6252            7.67m\n",
      "         3           0.5838            7.59m\n",
      "         4           0.5489            7.51m\n",
      "         5           0.5229            7.45m\n",
      "         6           0.5005            7.37m\n",
      "         7           0.4783            7.27m\n",
      "         8           0.4611            7.19m\n",
      "         9           0.4429            7.12m\n",
      "        10           0.4301            7.05m\n",
      "        20           0.3329            6.21m\n",
      "        30           0.2765            5.39m\n",
      "        40           0.2396            4.68m\n",
      "        50           0.2078            3.92m\n",
      "        60           0.1882            3.15m\n",
      "        70           0.1721            2.45m\n",
      "        80           0.1552            1.65m\n",
      "        90           0.1423           52.73s\n",
      "       100           0.1343            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6868            7.16m\n",
      "         2           0.6281            7.80m\n",
      "         3           0.5841            8.80m\n",
      "         4           0.5523            9.75m\n",
      "         5           0.5255            9.45m\n",
      "         6           0.5005            8.91m\n",
      "         7           0.4794            8.64m\n",
      "         8           0.4623            8.90m\n",
      "         9           0.4473            8.99m\n",
      "        10           0.4341            8.83m\n",
      "        20           0.3366            6.97m\n",
      "        30           0.2793            5.62m\n",
      "        40           0.2424            4.64m\n",
      "        50           0.2153            3.76m\n",
      "        60           0.1952            2.97m\n",
      "        70           0.1800            2.26m\n",
      "        80           0.1673            1.51m\n",
      "        90           0.1493           45.52s\n",
      "       100           0.1391            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6854            7.41m\n",
      "         2           0.6286            7.25m\n",
      "         3           0.5867            7.36m\n",
      "         4           0.5519            7.01m\n",
      "         5           0.5262            6.79m\n",
      "         6           0.5035            6.60m\n",
      "         7           0.4833            6.57m\n",
      "         8           0.4655            6.45m\n",
      "         9           0.4508            6.44m\n",
      "        10           0.4389            6.40m\n",
      "        20           0.3427            5.54m\n",
      "        30           0.2862            4.73m\n",
      "        40           0.2474            4.05m\n",
      "        50           0.2178            3.35m\n",
      "        60           0.1939            2.67m\n",
      "        70           0.1786            1.99m\n",
      "        80           0.1661            1.32m\n",
      "        90           0.1487           39.71s\n",
      "       100           0.1391            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6836            6.68m\n",
      "         2           0.6262            6.63m\n",
      "         3           0.5837            6.56m\n",
      "         4           0.5482            6.48m\n",
      "         5           0.5193            6.41m\n",
      "         6           0.4967            6.33m\n",
      "         7           0.4769            6.25m\n",
      "         8           0.4579            6.17m\n",
      "         9           0.4428            6.19m\n",
      "        10           0.4286            6.17m\n",
      "        20           0.3329            5.39m\n",
      "        30           0.2794            4.57m\n",
      "        40           0.2392            3.87m\n",
      "        50           0.2118            3.19m\n",
      "        60           0.1921            2.57m\n",
      "        70           0.1764            1.96m\n",
      "        80           0.1601            1.31m\n",
      "        90           0.1454           39.48s\n",
      "       100           0.1363            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6846            6.24m\n",
      "         2           0.6258            6.16m\n",
      "         3           0.5845            6.09m\n",
      "         4           0.5497            6.04m\n",
      "         5           0.5228            5.96m\n",
      "         6           0.4993            5.89m\n",
      "         7           0.4803            5.83m\n",
      "         8           0.4610            5.77m\n",
      "         9           0.4460            5.70m\n",
      "        10           0.4329            5.65m\n",
      "        20           0.3364            5.41m\n",
      "        30           0.2776            4.94m\n",
      "        40           0.2421            4.19m\n",
      "        50           0.2138            3.44m\n",
      "        60           0.1906            2.70m\n",
      "        70           0.1735            1.98m\n",
      "        80           0.1626            1.32m\n",
      "        90           0.1471           39.82s\n",
      "       100           0.1372            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6837            5.73m\n",
      "         2           0.6241            6.71m\n",
      "         3           0.5825            6.24m\n",
      "         4           0.5506            6.06m\n",
      "         5           0.5238            5.91m\n",
      "         6           0.4971            5.79m\n",
      "         7           0.4779            5.63m\n",
      "         8           0.4580            5.74m\n",
      "         9           0.4433            5.59m\n",
      "        10           0.4275            5.56m\n",
      "        20           0.3315            4.86m\n",
      "        30           0.2768            4.08m\n",
      "        40           0.2375            3.42m\n",
      "        50           0.2107            2.81m\n",
      "        60           0.1894            2.22m\n",
      "        70           0.1744            1.67m\n",
      "        80           0.1586            1.11m\n",
      "        90           0.1421           33.15s\n",
      "       100           0.1335            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6853            5.37m\n",
      "         2           0.6256            5.33m\n",
      "         3           0.5815            5.26m\n",
      "         4           0.5489            5.20m\n",
      "         5           0.5225            5.14m\n",
      "         6           0.4956            5.09m\n",
      "         7           0.4772            5.03m\n",
      "         8           0.4589            4.98m\n",
      "         9           0.4418            4.92m\n",
      "        10           0.4275            4.87m\n",
      "        20           0.3280            4.31m\n",
      "        30           0.2733            3.77m\n",
      "        40           0.2367            3.23m\n",
      "        50           0.2070            2.69m\n",
      "        60           0.1878            2.14m\n",
      "        70           0.1723            1.60m\n",
      "        80           0.1616            1.06m\n",
      "        90           0.1437           31.93s\n",
      "       100           0.1349            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6833            5.33m\n",
      "         2           0.6235            5.26m\n",
      "         3           0.5819            5.21m\n",
      "         4           0.5466            5.15m\n",
      "         5           0.5195            5.10m\n",
      "         6           0.4958            5.05m\n",
      "         7           0.4771            5.00m\n",
      "         8           0.4575            4.95m\n",
      "         9           0.4402            4.91m\n",
      "        10           0.4269            4.86m\n",
      "        20           0.3296            4.31m\n",
      "        30           0.2749            3.77m\n",
      "        40           0.2359            3.22m\n",
      "        50           0.2094            2.71m\n",
      "        60           0.1895            2.16m\n",
      "        70           0.1727            1.61m\n",
      "        80           0.1607            1.09m\n",
      "        90           0.1418           32.58s\n",
      "       100           0.1333            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.6871            6.35m\n",
      "         2           0.6294            7.06m\n",
      "         3           0.5889            7.24m\n",
      "         4           0.5541            7.19m\n",
      "         5           0.5275            6.70m\n",
      "         6           0.5028            6.37m\n",
      "         7           0.4816            6.13m\n",
      "         8           0.4649            5.93m\n",
      "         9           0.4516            5.79m\n",
      "        10           0.4361            5.69m\n",
      "        20           0.3396            4.87m\n",
      "        30           0.2853            4.14m\n",
      "        40           0.2448            3.52m\n",
      "        50           0.2176            2.89m\n",
      "        60           0.1959            2.32m\n",
      "        70           0.1807            1.74m\n",
      "        80           0.1685            1.16m\n",
      "        90           0.1487           34.36s\n",
      "       100           0.1398            0.00s\n"
     ]
    }
   ],
   "source": [
    "score = np.mean(cross_val_score(GradientBoostingClassifier(n_estimators=100, verbose=True), \n",
    "                                texts_matrix, labels, cv=10, scoring='f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88512843717957623"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf + DecisionTreeClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    1.9s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    4.3s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    2.1s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    1.8s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    1.8s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    1.8s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    1.8s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    1.7s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    1.7s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    1.7s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    1.6s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "score = np.mean(cross_val_score(RandomForestClassifier(n_estimators=100, verbose=True), \n",
    "                                texts_matrix_tfidf, labels, cv=10, scoring='f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91385408988093297"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf + DecisionTreeClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = np.mean(cross_val_score(DecisionTreeClassifier(), \n",
    "                                texts_matrix_tfidf, labels, cv=10, scoring='f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8923321061843964"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C tfidf random rofest работает лучше, gradient bossting'у все равно, DecisionTreeClassifier стал чуть-чуть лучше. \n",
    "Но все равно это все не очень, хуже, чем просто logisticRegression из коробки. Поэтому попробуем лучше подобрать параметры к лучшему на данный момент классификатору: LogisticRegression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV for LogisticRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned_parameters = {'tol': [1e-6, 1e-5, 1e-4, 1e-3],\n",
    "                     'C': [0.1, 0.5, 1, 7, 10, 50, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(LogisticRegression(), tuned_parameters, cv=10, scoring='f1', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:   39.7s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks       | elapsed:  2.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 28 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 280 out of 280 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.1, 0.5, 1, 7, 10, 50, 100], 'tol': [1e-06, 1e-05, 0.0001, 0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='f1', verbose=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(texts_matrix, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9424520527834863\n"
     ]
    }
   ],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воо, стало лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #11 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из работы с \\*граммами, использование iграмм, где i>1, неэффективно. Также после подбора лучшего классификатора и параметров (конечно, перебор был не очень большим, но можно все же), CountVectorizer + LogisticRegression оказались лучшшей парой. Это значит, видимо, что в спаме присутствуют характерные слова, под кторые хорошо подстраивается функция логистической регрессии. При этом характерных пар и троек слов уже намного меньше, что видно по экспериментам с *граммами. Эти догадки также подтверждает пункт 6 --- там мы смотрели на результат предсказания lr на 5 сообщениях --- первые 4 он определил верно, пятое --- нет. В первых двух сообщениях (спамах) можно видеть повторяющиеся слова, которые, видимо, и отлавливает lr. В последнем же сообщении всего 2 слова, и среди них, видимо, нет слов, которые lr в процессе обучения выделила как ключевые в спаме (выделила -- построила функцию, сильно на них опирающуюся). \n",
    "\n",
    "Может, в неспаме этой выборки также присутствуют характерные слова --- например, \"привет\" или тп, т.е. слова, характерные для диалогов (кажется, неспам этого датасета преимущественно состоит из диалгов). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
